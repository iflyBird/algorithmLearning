# 逻辑回归（本身算法比较弱）
- 在二分类的情况下，输出0或1.那拥有这类性质的函数称为海维赛德阶跃函数，又称之为单位阶跃函数
- 单位阶跃函数的问题在于：在0点位置该函数提哦啊越过澄很难处理，幸运的是，sigmoid函数也有类似的型制造，且数学上更容易处理
- sigmoid函数公式:
- 逻辑回归就是把sigmoid函数和线性模型结合后得到的
- 二项逻辑回归,多项逻辑回归
- 线性回归要求输入和输出y是连续性数值，逻辑回归要求是分类型变量
- 线性回归和逻辑回归都是线性模型，线性回归本质是回归不是分类，他要做的是你和阉割版您构成的直线，逻辑回归是分类模型，要求x线性可分，但x和y没有线性的的关系
- 线性回归是直接为分析x和y的关系，逻辑回归是分析y取某个值的概率和x的关系
- 损失函数衡量模型的的好坏
- 当损失是过于小的时候,也就是模型你和大部分的数据,知识后就容易出现过拟合，为了防止过拟合，我们会引入正则化
# 梯度下降法
- 1、先决条件，确认优化模型的假设函数和损失函数
- 2、算法相关系数的初始化
- 3、算法过程
# 梯度下降的种类
- 批量梯度下降法bgd（所有样本）
- 随意梯度下降法sgd(速度快)
- 随机梯度下降法依次迭代一个样本，导致迭代方向变化很大，不能很快的收敛局部得到最优解，随机梯度下降法在处理非凸函数优化的过程中有非常好的表现，由于
其下降的方向具有一定的随机性
- 小批量梯度下降法mbgd
- 是批量数据下降法和随机梯度下降法的折中。
- 一般在深度学习中,可以采用这种方法，将数据一个batch一个batch的送进去训练
- 上述三种方法存在问题:<br/>
 1、选择合适的学习率<br/>
 2、如何对参数选择合适的学习率
## 梯度下降的算法调优
- 算法的步长
- 算法参数的初始值的选择
- 标准化
# 准备数据
- 使用可用特征的均值来填补缺失值
- 使用特殊值来填补缺失值
- 忽略有缺失的的样本
- 使用相似样本的均值填补缺失值
- 使用另外的机器学习算法预测缺失值
## 数据预处理
- 如果测试集中一条数据的特征值已经确实，那么虚选择实数0来替换所有缺失值,
- 如果测试集中一条数据的类别标签已经缺失，那么我们将该类别数据丢弃，
    